{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning Loop with DQN\n",
    "\n",
    "Although DQn is not the algoritm for the continuous action spaces, but this tutorial helps to understand how to apply RL loops to Aircontrol in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "import PIL.Image as Image\n",
    "from collections import deque\n",
    "import base64\n",
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from io import BytesIO\n",
    "from matplotlib.pyplot import  imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import matplotlib.animation as animation\n",
    "import json\n",
    "import socket\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import random\n",
    "# import dtale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "torch.use_deterministic_algorithms(False)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import airctrl\n",
    "from airctrl import environment \n",
    "from airctrl import sample_generator\n",
    "from dqn_agent import Agent\n",
    "from airctrl.utils import unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2.1\n"
     ]
    }
   ],
   "source": [
    "# AirControl Pypi Version\n",
    "print(airctrl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WandB to track experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"cesna-150\",\n",
    "           config={\n",
    "               \"batch_size\": 16,\n",
    "               \"loss\":\"mse\",\n",
    "               \"gamma\": 0.9 ,\n",
    "               \"tau\" : 0.001 ,\n",
    "               \"lr\" : 0.0001\n",
    "            },\n",
    "            notes='mse loss, lidar added, tau changed, gamma chaaged , LSTM', \n",
    "            mode=\"disabled\") # change to online to see plots on web portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"WANDB_SILENT\"] = \"True\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Unity env from build\n",
    "L = unity.Launch()\n",
    "process = L.launch_executable(\"/home/supatel/Games/AirControl_2020/Build/Linux/v1.2.0-AirControl.x86_64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now call method `.get_connected()` to get connected\n"
     ]
    }
   ],
   "source": [
    "# Define env variables\n",
    "sample = sample_generator.samples()\n",
    "env =  environment.Trigger()\n",
    "agent = Agent(state_size=384, action_size=4, seed=0, batch_size=64,gamma=0.9,tau=0.01,lr=0.0001)\n",
    "MAX_ENV_SIZE = 10000 #max distance in any direction agent can travel\n",
    "MAX_EULAR = 360 # boundary for eular\n",
    "MAX_LIDAR_RANGE = 500\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_reset():\n",
    "    output = env.reset(IsOutput=True)\n",
    "    env.set_ui(ShowUIElements=True, IsActive= True)\n",
    "    env.set_camera(ActiveCamera=1, IsActive=True, IsCapture=False, CaptureCamera=1, CaptureType=0,CaptureHeight=540, CaptureWidth=960)\n",
    "    env.set_lidar(IsActive=False)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_Features(output):\n",
    "    \"\"\"\n",
    "    Convert output from env to features\n",
    "    \"\"\"\n",
    "    MSL = output['MSL']/MAX_ENV_SIZE\n",
    "    Latitude = output['Latitude']\n",
    "    Longitude = output['Longitude']\n",
    "    normalizedRPM =  output['CurrentRPM']/output['MaxRPM']\n",
    "    normalizedPower =  output['CurrentPower']/output['MaxPower']\n",
    "    normalizedSpeed = output['CurrentSpeed']/150 # Normlizing by max speed\n",
    "    pitchAngle = output['PitchAngle']\n",
    "    bankAngle = output['BankAngle']\n",
    "    ifCollision = output['IfCollision']\n",
    "    collisionObject = output['CollisionObject']\n",
    "    Reward = output[\"Reward\"]/MAX_ENV_SIZE # Normalizing rewards\n",
    "    IsGrounded = 1.0 if(output[\"IsGrounded\"]) else 0.0\n",
    "    IsFlying = 1.0 if(output[\"IsFlying\"]) else 0.0\n",
    "    IsTaxiing = 1.0 if(output[\"IsTaxiing\"]) else 0.0\n",
    "    PosXAbs = (output[\"PosXAbs\"])/MAX_ENV_SIZE\n",
    "    PosYAbs = (output[\"PosYAbs\"])/MAX_ENV_SIZE\n",
    "    PosZAbs = (output[\"PosZAbs\"])/MAX_ENV_SIZE\n",
    "    PosXRel = (output[\"PosXRel\"])/MAX_ENV_SIZE\n",
    "    PosYRel = (output[\"PosYRel\"])/MAX_ENV_SIZE\n",
    "    PosZRel = (output[\"PosZRel\"])/MAX_ENV_SIZE\n",
    "    RotXAbs = (output[\"RotXAbs\"])/MAX_EULAR\n",
    "    RotYAbs = (output[\"RotYAbs\"])/MAX_EULAR\n",
    "    RotZAbs = (output[\"RotZAbs\"])/MAX_EULAR\n",
    "    RotXRel = (output[\"RotXRel\"])/MAX_EULAR\n",
    "    RotYRel = (output[\"RotYRel\"])/MAX_EULAR\n",
    "    RotZRel = (output[\"RotZRel\"])/MAX_EULAR\n",
    "    LidarPointCloud = 1.0-np.asarray(output['LidarPointCloud'])/MAX_LIDAR_RANGE\n",
    "\n",
    "    feature_vector = [MSL, Latitude, Longitude, normalizedRPM, normalizedPower, normalizedSpeed, pitchAngle, \\\n",
    "                      bankAngle, IsGrounded, IsFlying, IsTaxiing, \\\n",
    "                     PosXAbs, PosYAbs, PosZAbs, PosXRel, PosYRel, PosZRel,RotXAbs,RotYAbs,RotZAbs,RotXRel,RotYRel,RotZRel, ifCollision] + LidarPointCloud.tolist()\n",
    "\n",
    "    return np.asarray(feature_vector),Reward, ifCollision,collisionObject\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountFrequency(my_list):\n",
    " \n",
    "    # Creating an empty dictionary\n",
    "    freq = {}\n",
    "    for item in my_list:\n",
    "        if (item in freq):\n",
    "            freq[item] += 1\n",
    "        else:\n",
    "            freq[item] = 1\n",
    "    per_freq = {}\n",
    "    for key,values in freq.items():\n",
    "        per_freq[key]=freq[key]/len(my_list)*100\n",
    "    return per_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Test the model (DQN if OFF policy RL Algo.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run():\n",
    "    # do a test run\n",
    "    msl = 0\n",
    "    env_output = env_reset()\n",
    "    state, reward, done, collision_object = output_to_Features(env_output)\n",
    "    for t in range(10000):\n",
    "        if state[0]>msl:\n",
    "            msl = state[0]\n",
    "        _, action = agent.act(state, 0.0)\n",
    "        pitch =  action[0]\n",
    "        yaw = action[1]\n",
    "        roll= action[2]\n",
    "        stickyThrottle=action[3]\n",
    "        env_output = env.step(Pitch=pitch, Yaw=yaw, Roll=roll, StickyThrottle=stickyThrottle)\n",
    "        next_state, reward, done, collision_object = output_to_Features(env_output)\n",
    "        state = next_state \n",
    "        wandb.log({\"M/score\": reward, \"M/max height\":msl})\n",
    "        wandb.log({\"M/Pitch\":pitch, \"M/Yaw\":yaw, \"M/Roll\":roll, \"M/StickyThrottle\":stickyThrottle})\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=5000, max_t=10000, eps_start=1.0, eps_end=0.05, eps_decay=0.999):\n",
    "    #     global dataframe\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=1000)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        if i_episode%20==0:\n",
    "            test_run()\n",
    "        env_output = env_reset()\n",
    "        state, reward, done, collision_object = output_to_Features(env_output)\n",
    "        score = 0\n",
    "        triggers = []\n",
    "        msl = 0\n",
    "        for t in range(max_t):\n",
    "            if state[0]>msl:\n",
    "                msl = state[0]\n",
    "            trigger_type,action = agent.act(state, eps)\n",
    "            triggers.append(trigger_type)\n",
    "            pitch =  action[0]\n",
    "            yaw = action[1]\n",
    "            roll= action[2]\n",
    "            stickyThrottle=action[3]\n",
    "            env_output = env.step(Pitch=pitch, Yaw=yaw, Roll=roll, StickyThrottle=stickyThrottle)\n",
    "            next_state, reward, done, collision_object = output_to_Features(env_output)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state \n",
    "            #             dataframe = dataframe.append(env_output, ignore_index=True)\n",
    "            wandb.log({\"Pitch\":pitch, \"Yaw\":yaw, \"Roll\":roll, \"StickyThrottle\":stickyThrottle})\n",
    "            \n",
    "            if done:\n",
    "                print(\"üîÅ Episode {0} | Counter: {1} | Collided with {2} | Score : {3} | reward : {4} | eps : {5}\".format(i_episode,env_output[\"Counter\"],collision_object, score, reward, eps))\n",
    "                wandb.log({\"eps\": eps, \"score\": score, \"score per iteration\" : score/(t+1e-6), \"episode size\":t,\"max height\":msl})\n",
    "                wandb.log(CountFrequency(triggers))\n",
    "                break\n",
    "            score += reward\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)    # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        \n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113025/2593937204.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_113025/3263954236.py\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m#             dataframe = dataframe.append(env_output, ignore_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Pitch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Yaw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0myaw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Roll\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"StickyThrottle\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstickyThrottle\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m ) -> Callable:\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must call wandb.init() before {}()\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
